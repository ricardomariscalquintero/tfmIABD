version: "3"

services:
  namenode:
    build: 
      context: .
      dockerfile: imagenes/hadoop/Dockerfile
    container_name: namenode
    hostname: namenode
    environment:
      - HDFS_NAMENODE_USER=root
    ports:
      - "9870:9870"   # HDFS Web UI
    networks:
      - hadoop-net
    volumes:
      - namenode_data:/tmp/hadoop
      - /tmp/shared:/shared
    command: >
      bash -c "
        if [ ! -f /tmp/hadoop/.hdfs_formatted ]; then
          hdfs namenode -format -force &&
          touch /tmp/hadoop/.hdfs_formatted
        fi
        hdfs namenode
      "

  resourcemanager:
    build: 
      context: .
      dockerfile: imagenes/hadoop/Dockerfile
    container_name: resourcemanager
    hostname: resourcemanager
    environment:
      - YARN_RESOURCEMANAGER_USER=root
    ports:
      - "8088:8088"   # YARN Web UI
    networks:
      - hadoop-net
    volumes:
      - /tmp/shared:/shared
    command: >
      bash -c "
        yarn resourcemanager
      "

  datanode1:
    build: 
      context: .
      dockerfile: imagenes/hadoop/Dockerfile
    container_name: datanode1
    hostname: datanode1
    environment:
      - HDFS_DATANODE_USER=root
      - YARN_NODEMANAGER_USER=root
    networks:
      - hadoop-net
    volumes:
      - datanode1_data:/tmp/hadoop
      - /tmp/shared:/shared
    depends_on:
      - namenode
      - resourcemanager
    command: >
      bash -c "
        hdfs datanode &
        yarn nodemanager &
        tail -f /dev/null
      "

  datanode2:
    build: 
      context: .
      dockerfile: imagenes/hadoop/Dockerfile
    container_name: datanode2
    hostname: datanode2
    environment:
      - HDFS_DATANODE_USER=root
      - YARN_NODEMANAGER_USER=root
    networks:
      - hadoop-net
    volumes:
      - datanode2_data:/tmp/hadoop
      - /tmp/shared:/shared
    depends_on:
      - namenode
      - resourcemanager
    command: >
      bash -c "
        hdfs datanode &
        yarn nodemanager &
        tail -f /dev/null
      "

  metastore-db:
    image: mysql:8.0
    container_name: metastore-db
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: metastore
      MYSQL_USER: hive
      MYSQL_PASSWORD: hive
    volumes:
      - metastore_data:/var/lib/mysql
      - /tmp/shared:/shared
      - ./imagenes/mysql/init-airflow.sql:/docker-entrypoint-initdb.d/init-airflow.sql
    ports:
      - "3306:3306"
    networks:
      - hadoop-net

  hive-server:
    build: 
      context: .
      dockerfile: imagenes/hive/Dockerfile
    container_name: hive-server
    hostname: hive-server
    environment:
      HIVE_HOME: /opt/hive
      HADOOP_HOME: /opt/hadoop
      HADOOP_CONF_DIR: /opt/hadoop/etc/hadoop
      HIVE_CONF_DIR: /opt/hive/conf
      SERVICE_PRECONDITION: "namenode:9870 metastore-db:3306"
    networks:
      - hadoop-net
    volumes:
      - hive_data:/tmp/hive
      - /tmp/shared:/shared
    depends_on:
      - namenode
      - datanode1
      - datanode2
      - resourcemanager
      - metastore-db
    ports:
      - "10000:10000"   # HiveServer2 (JDBC/Beeline)
      - "9083:9083"     # Hive Metastore (para Thrift)
    command: >
      bash -c "
        dockerize -wait tcp://metastore-db:3306 -timeout 60s &&
        if [ ! -f /tmp/hive/.hive_initialized ]; then
          schematool -dbType mysql -initSchema --verbose &&
          touch /tmp/hive/.hive_initialized
        fi
        hive --service metastore &
        hive --service hiveserver2 &
        tail -f /dev/null
      "

  spark-client:
    build: 
      context: .
      dockerfile: imagenes/spark/Dockerfile               
    container_name: spark-client
    hostname: spark-client
    networks:
      - hadoop-net
    ports:
      - "8888:8888"
    depends_on:
      - namenode
      - datanode1
      - datanode2
      - resourcemanager
      - hive-server
    volumes:
      - /tmp/shared:/shared        
    environment:
      - SPARK_HOME=/opt/spark
      - HADOOP_HOME=/opt/hadoop
      - HIVE_HOME=/opt/hive
      - PATH=/opt/spark/bin:/opt/hadoop/bin:/opt/hive/bin:$PATH
      - SPARK_LOG_LEVEL=ERROR
    tty: true

  airflow:
    build: 
      context: .
      dockerfile: imagenes/airflow/Dockerfile
    container_name: airflow
    restart: always
    depends_on:
      - metastore-db
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=mysql+pymysql://airflow:airflow@metastore-db:3306/airflow
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=mysql+pymysql://airflow:airflow@metastore-db:3306/airflow
      - AIRFLOW__CORE__FERNET_KEY=algo_generado
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ./imagenes/airflow/dags:/opt/airflow/dags
      - ./imagenes/airflow/start-airflow.sh:/opt/airflow/start-airflow.sh
    ports:
      - "8080:8080"
    entrypoint: /bin/bash  # << Esto evita que interprete `command:` como airflow
    command: /opt/airflow/start-airflow.sh
    networks:
      - hadoop-net

volumes:
  namenode_data:
  datanode1_data:
  datanode2_data:
  metastore_data:
  hive_data:
  airflow_data:

networks:
  hadoop-net:


version: "3"

services:
  namenode:
    build: 
      context: .
      dockerfile: imagenes/hadoop/Dockerfile
    container_name: namenode
    hostname: namenode
    environment:
      - HDFS_NAMENODE_USER=root
    ports:
      - "9870:9870"  # Web UI
      - "9000:9000"  # RPC para clientes HDFS
    networks:
      - hadoop-net
    volumes:
      - namenode_data:/tmp/hadoop
      - /tmp/shared:/shared
    command: >
      bash -c "
        if [ ! -f /tmp/hadoop/.hdfs_formatted ]; then
          hdfs namenode -format -force &&
          touch /tmp/hadoop/.hdfs_formatted
        fi
        hdfs namenode
      "

  resourcemanager:
    build: 
      context: .
      dockerfile: imagenes/hadoop/Dockerfile
    container_name: resourcemanager
    hostname: resourcemanager
    environment:
      - YARN_RESOURCEMANAGER_USER=root
    ports:
      - "8088:8088"   # YARN Web UI
    networks:
      - hadoop-net
    volumes:
      - /tmp/shared:/shared
    command: >
      bash -c "
        yarn resourcemanager
      "

  datanode1:
    build: 
      context: .
      dockerfile: imagenes/hadoop/Dockerfile
    container_name: datanode1
    hostname: datanode1
    environment:
      - HDFS_DATANODE_USER=root
      - YARN_NODEMANAGER_USER=root
    ports:
      - "9864:9864"  # WebHDFS del datanode1 (host → contenedor)
    networks:
      - hadoop-net
    volumes:
      - datanode1_data:/tmp/hadoop
      - /tmp/shared:/shared
    depends_on:
      - namenode
      - resourcemanager
    command: >
      bash -c "
        hdfs datanode &
        yarn nodemanager &
        tail -f /dev/null
      "

  datanode2:
    build: 
      context: .
      dockerfile: imagenes/hadoop/Dockerfile
    container_name: datanode2
    hostname: datanode2
    environment:
      - HDFS_DATANODE_USER=root
      - YARN_NODEMANAGER_USER=root
    ports:
      - "9865:9864"  # WebHDFS del datanode2 (host:9865 → contenedor:9864)
    networks:
      - hadoop-net
    volumes:
      - datanode2_data:/tmp/hadoop
      - /tmp/shared:/shared
    depends_on:
      - namenode
      - resourcemanager
    command: >
      bash -c "
        hdfs datanode &
        yarn nodemanager &
        tail -f /dev/null
      "

  metastore-db:
    image: mysql:8.0
    container_name: metastore-db
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: metastore
      MYSQL_USER: hive
      MYSQL_PASSWORD: hive
    volumes:
      - metastore_data:/var/lib/mysql
      - /tmp/shared:/shared
      - ./imagenes/mysql/init-airflow.sql:/docker-entrypoint-initdb.d/init-airflow.sql
    ports:
      - "3306:3306"
    networks:
      - hadoop-net

  hive-server:
    build: 
      context: .
      dockerfile: imagenes/hive/Dockerfile
    container_name: hive-server
    hostname: hive-server
    environment:
      HIVE_HOME: /opt/hive
      HADOOP_HOME: /opt/hadoop
      HADOOP_CONF_DIR: /opt/hadoop/etc/hadoop
      HIVE_CONF_DIR: /opt/hive/conf
      SERVICE_PRECONDITION: "namenode:9870 metastore-db:3306"
    networks:
      - hadoop-net
    volumes:
      - hive_data:/tmp/hive
      - /tmp/shared:/shared
    depends_on:
      - namenode
      - datanode1
      - datanode2
      - resourcemanager
      - metastore-db
    ports:
      - "10000:10000"   # HiveServer2 (JDBC/Beeline)
      - "9083:9083"     # Hive Metastore (para Thrift)
    command: >
      bash -c "
        dockerize -wait tcp://metastore-db:3306 -timeout 60s &&
        if [ ! -f /tmp/hive/.hive_initialized ]; then
          schematool -dbType mysql -initSchema --verbose &&
          touch /tmp/hive/.hive_initialized
        fi
        hive --service metastore &
        hive --service hiveserver2 &
        tail -f /dev/null
      "

  spark-client:
    build: 
      context: .
      dockerfile: imagenes/spark/Dockerfile               
    container_name: spark-client
    hostname: spark-client
    networks:
      - hadoop-net
    ports:
      - "8888:8888"
      - "2222:22"
    depends_on:
      - namenode
      - datanode1
      - datanode2
      - resourcemanager
      - hive-server
    volumes:
      - /tmp/shared:/shared
      - ./imagenes/spark/scripts:/opt/spark/scripts
      - ./notebooks:/notebooks                  # Carpeta permanente para los cuadernillos Jupyter
      - /tmp/.X11-unix:/tmp/.X11-unix           # Para ver la ventana en pygame
        
    environment:
      - SPARK_HOME=/opt/spark
      - HADOOP_HOME=/opt/hadoop
      - HIVE_HOME=/opt/hive
      - PATH=/opt/spark/bin:/opt/hadoop/bin:/opt/hive/bin:$PATH
      - SPARK_LOG_LEVEL=ERROR
      - DISPLAY=${DISPLAY}                      # Para ver la ventana en pygame
    tty: true

  airflow:
    build: 
      context: .
      dockerfile: imagenes/airflow/Dockerfile
    container_name: airflow
    restart: always
    depends_on:
      - metastore-db
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=mysql+pymysql://airflow:airflow@metastore-db:3306/airflow
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=mysql+pymysql://airflow:airflow@metastore-db:3306/airflow
      - AIRFLOW__CORE__FERNET_KEY=algo_generado
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ./imagenes/airflow/dags:/opt/airflow/dags
      - ./imagenes/airflow/partidas:/opt/airflow/partidas
      - ./imagenes/airflow/start-airflow.sh:/opt/airflow/start-airflow.sh
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8080:8080"
    entrypoint: /bin/bash  # << Esto evita que interprete `command:` como airflow
    command: /opt/airflow/start-airflow.sh
    networks:
      - hadoop-net

  dremio:
    image: dremio/dremio-oss:26.0.0
    container_name: dremio
    ports:
      - "9047:9047"     # Web UI
      - "31010:31010"   # JDBC/ODBC
      - "32010:32010"   # Arrow Power BI
      - "45678:45678"   # backend
    volumes:
      - dremio_data:/opt/dremio/data
    networks:
      - hadoop-net

volumes:
  namenode_data:
  datanode1_data:
  datanode2_data:
  metastore_data:
  hive_data:
  airflow_data:
  dremio_data:

networks:
  hadoop-net:
#    external: true


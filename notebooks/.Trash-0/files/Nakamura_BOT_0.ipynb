{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc69900-9fdb-4188-846c-5a649be2149f",
   "metadata": {},
   "source": [
    "## Nueva filosofía"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "134de9b2-cf56-4ca0-99b8-8ce878bf53c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, input_file_name\n",
    "from pyspark.sql.types import StringType\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a90933e2-a6d2-4e9c-aebb-d8a92d5b0a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/10 17:50:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"CargarPartidas\")\n",
    "    .master(\"local[*]\")  # \"yarn\"\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2545dae-23a0-4706-89af-3d63764c28a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/ajedrez/jugador\n",
      "Found 1 items\n",
      "drwxr-xr-x   - root supergroup          0 2025-05-09 11:46 /user/ajedrez/raw\n"
     ]
    }
   ],
   "source": [
    "# Por si se quiere eliminar alguna carpeta\n",
    "!hdfs dfs -rm -r /user/ajedrez/jugador\n",
    "# En principio, solo con la carpeta raw y las partidas dentro nos vale\n",
    "!hdfs dfs -ls /user/ajedrez/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b74c2df-3e02-4fd9-a3f1-47ff3a16e59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Leer todos los archivos como (ruta, contenido)\n",
    "rdd = spark.sparkContext.wholeTextFiles(\"hdfs:///user/ajedrez/raw/*.pgn\")\n",
    "\n",
    "# Dividir cada archivo en partidas\n",
    "def dividir_partidas_por_archivo(nombre_y_contenido):\n",
    "    ruta, contenido = nombre_y_contenido\n",
    "    partidas = re.split(r'\\n(?=\\[Event )', contenido)\n",
    "    return [(ruta, pgn) for pgn in partidas if \"[Event\" in pgn]\n",
    "\n",
    "rdd_partidas = rdd.flatMap(dividir_partidas_por_archivo)\n",
    "\n",
    "# Convertir a DataFrame\n",
    "df = rdd_partidas.toDF([\"archivo\", \"pgn\"])\n",
    "\n",
    "# UDFs para extraer tags\n",
    "def extraer_tag(tag, texto):\n",
    "    match = re.search(rf'\\[{tag} \"([^\"]+)\"\\]', texto)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "extraer_white = udf(lambda x: extraer_tag(\"White\", x), StringType())\n",
    "extraer_black = udf(lambda x: extraer_tag(\"Black\", x), StringType())\n",
    "extraer_date = udf(lambda x: extraer_tag(\"Date\", x), StringType())\n",
    "\n",
    "df = df.withColumn(\"white\", extraer_white(col(\"pgn\"))) \\\n",
    "       .withColumn(\"black\", extraer_black(col(\"pgn\"))) \\\n",
    "       .withColumn(\"date\", extraer_date(col(\"pgn\")))\n",
    "\n",
    "\n",
    "# Partidas donde Nakamura juega con blancas\n",
    "nakamura_white_df = df.filter(col(\"white\") == \"Nakamura,Hi\")\n",
    "\n",
    "# Partidas donde Nakamura juega con negras\n",
    "nakamura_black_df = df.filter(col(\"black\") == \"Nakamura,Hi\")\n",
    "\n",
    "# Guardar en HDFS\n",
    "# nakamura_white_df.select(\"pgn\").write.mode(\"overwrite\").text(\"hdfs:///user/ajedrez/jugador/Nakamura_blancas\")\n",
    "# nakamura_black_df.select(\"pgn\").write.mode(\"overwrite\").text(\"hdfs:///user/ajedrez/jugador/Nakamura_negras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e83401-bbca-4b47-83f9-822eb447b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En principio, solo con la carpeta raw y las partidas dentro nos vale\n",
    "# !hdfs dfs -ls /user/ajedrez/jugador/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f639b90-2fb8-4108-9498-be9cf0d7524a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (35, 5)\n",
      "['archivo', 'pgn', 'white', 'black', 'date']\n"
     ]
    }
   ],
   "source": [
    "filas = nakamura_white_df.count()\n",
    "columnas = len(nakamura_white_df.columns)\n",
    "print(f\"Shape: ({filas}, {columnas})\")\n",
    "\n",
    "print(nakamura_white_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ea8cc4-6fcd-40a7-9b07-0f32135f4667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+----------+\n",
      "|white      |black           |date      |\n",
      "+-----------+----------------+----------+\n",
      "|Nakamura,Hi|Vidit,S         |2024.04.05|\n",
      "|Nakamura,Hi|Praggnanandhaa,R|2024.04.07|\n",
      "|Nakamura,Hi|Nevednichy,V    |2024.04.02|\n",
      "|Nakamura,Hi|Eljanov,P       |2024.04.02|\n",
      "|Nakamura,Hi|Kuzubov,Y       |2024.04.02|\n",
      "|Nakamura,Hi|Kovalev,Vl      |2024.04.02|\n",
      "|Nakamura,Hi|Duda,J          |2024.04.02|\n",
      "|Nakamura,Hi|Makarian,Rudik  |2024.04.02|\n",
      "|Nakamura,Hi|Richter,Paul    |2024.04.02|\n",
      "|Nakamura,Hi|Durarbayli,Vasif|2024.04.02|\n",
      "+-----------+----------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nakamura_white_df.select(\"white\", \"black\", \"date\").show(truncate=False, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73fc26c-2342-47e8-a7f1-5b68468c9e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.select(\"pgn\").first()[\"pgn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25554bf2-ef29-4f77-a94f-b01f95b3b53a",
   "metadata": {},
   "source": [
    "### Se puede arreglar eso de separar en dos dataframes para luego unirlos en 1 es mejor no separarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "174ff74c-e519-4a9b-9508-196de716852a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------+----+-----+\n",
      "|FEN                                                                 |Move|Color|\n",
      "+--------------------------------------------------------------------+----+-----+\n",
      "|rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1            |e2e4|white|\n",
      "|rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2        |g1f3|white|\n",
      "|r1bqkbnr/pppp1ppp/2n5/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3    |f1b5|white|\n",
      "|r1bqkb1r/pppp1ppp/2n2n2/1B2p3/4P3/5N2/PPPP1PPP/RNBQK2R w KQkq - 4 4 |d2d3|white|\n",
      "|r1bqk2r/pppp1ppp/2n2n2/1Bb1p3/4P3/3P1N2/PPP2PPP/RNBQK2R w KQkq - 1 5|c2c3|white|\n",
      "|r1bq1rk1/pppp1ppp/2n2n2/1Bb1p3/4P3/2PP1N2/PP3PPP/RNBQK2R w KQ - 1 6 |e1g1|white|\n",
      "|r1bq1rk1/ppp2ppp/2np1n2/1Bb1p3/4P3/2PP1N2/PP3PPP/RNBQ1RK1 w - - 0 7 |h2h3|white|\n",
      "|r1bq1rk1/ppp1nppp/3p1n2/1Bb1p3/4P3/2PP1N1P/PP3PP1/RNBQ1RK1 w - - 1 8|d3d4|white|\n",
      "|r1bq1rk1/pp2nppp/2pp1n2/1Bb1p3/3PP3/2P2N1P/PP3PP1/RNBQ1RK1 w - - 0 9|b5d3|white|\n",
      "|r1bq1rk1/pp2nppp/1bpp1n2/4p3/3PP3/2PB1N1P/PP3PP1/RNBQ1RK1 w - - 2 10|d4e5|white|\n",
      "|r1bq1rk1/pp2nppp/1bp2n2/4p3/4P3/2PB1N1P/PP3PP1/RNBQ1RK1 w - - 0 11  |f3e5|white|\n",
      "|r2q1rk1/pp2nppp/1bp2n2/4N3/4P3/2PB3b/PP3PP1/RNBQ1RK1 w - - 0 12     |e5c4|white|\n",
      "|r2q1rk1/pp2nppp/1bp2n2/8/2N1P1b1/2PB4/PP3PP1/RNBQ1RK1 w - - 2 13    |d1c2|white|\n",
      "|r2q1rk1/ppb1nppp/2p2n2/8/2N1P1b1/2PB4/PPQ2PP1/RNB2RK1 w - - 4 14    |e4e5|white|\n",
      "|r2q1rk1/ppbnnppp/2p5/4P3/2N3b1/2PB4/PPQ2PP1/RNB2RK1 w - - 1 15      |d3h7|white|\n",
      "|r2q1r1k/ppbnnppB/2p5/4P3/2N3b1/2P5/PPQ2PP1/RNB2RK1 w - - 1 16       |h7d3|white|\n",
      "|r2q1r1k/p1bnnpp1/2p5/1p2P3/2N3b1/2PB4/PPQ2PP1/RNB2RK1 w - - 0 17    |c4e3|white|\n",
      "|r2q1r1k/p1b1npp1/2p5/1p2n3/6b1/2PBN3/PPQ2PP1/RNB2RK1 w - - 0 18     |d3e2|white|\n",
      "|r2q1r1k/p1b1n1p1/2p5/1p2np2/6b1/2P1N3/PPQ1BPP1/RNB2RK1 w - - 0 19   |f2f4|white|\n",
      "|r2q1r1k/p3n1p1/1bp5/1p2np2/5Pb1/2P1N3/PPQ1B1P1/RNB2RK1 w - - 1 20   |g1f2|white|\n",
      "+--------------------------------------------------------------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------------------------------------------------------------+----+-----+\n",
      "|FEN                                                                |Move|Color|\n",
      "+-------------------------------------------------------------------+----+-----+\n",
      "|rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1         |c7c5|black|\n",
      "|rnbqkbnr/pp1ppppp/8/2p5/4P3/5N2/PPPP1PPP/RNBQKB1R b KQkq - 1 2     |d7d6|black|\n",
      "|rnbqkbnr/pp2pppp/3p4/2p5/3PP3/5N2/PPP2PPP/RNBQKB1R b KQkq - 0 3    |c5d4|black|\n",
      "|rnbqkbnr/pp2pppp/3p4/8/3NP3/8/PPP2PPP/RNBQKB1R b KQkq - 0 4        |g8f6|black|\n",
      "|rnbqkb1r/pp2pppp/3p1n2/8/3NP3/2N5/PPP2PPP/R1BQKB1R b KQkq - 2 5    |e7e5|black|\n",
      "|rnbqkb1r/pp3ppp/3p1n2/1B2p3/3NP3/2N5/PPP2PPP/R1BQK2R b KQkq - 1 6  |b8d7|black|\n",
      "|r1bqkb1r/pp1n1ppp/3p1n2/1B2pN2/4P3/2N5/PPP2PPP/R1BQK2R b KQkq - 3 7|a7a6|black|\n",
      "|r1bqkb1r/1p1n1ppp/p2p1n2/4pN2/B3P3/2N5/PPP2PPP/R1BQK2R b KQkq - 1 8|b7b5|black|\n",
      "|r1bqkb1r/3n1ppp/p2p1n2/1p2pN2/4P3/1BN5/PPP2PPP/R1BQK2R b KQkq - 1 9|d7c5|black|\n",
      "|r1bqkb1r/5ppp/p2p1n2/1pn1pNB1/4P3/1BN5/PPP2PPP/R2QK2R b KQkq - 3 10|c8f5|black|\n",
      "|r2qkb1r/5ppp/p2p1n2/1pn1pPB1/8/1BN5/PPP2PPP/R2QK2R b KQkq - 0 11   |f8e7|black|\n",
      "|r2qk2r/4bppp/p2p1B2/1pn1pP2/8/1BN5/PPP2PPP/R2QK2R b KQkq - 0 12    |e7f6|black|\n",
      "|r2qk2r/5ppp/p2p1b2/1pn1pP2/8/1BN5/PPP2PPP/R2Q1RK1 b kq - 1 13      |e5e4|black|\n",
      "|r2qk2r/5ppp/p2p1b2/1pn2P2/4N3/1B6/PPP2PPP/R2Q1RK1 b kq - 0 14      |c5e4|black|\n",
      "|r2qk2r/5ppp/p2p1b2/1p3P2/4n3/1B6/PPP2PPP/R2QR1K1 b kq - 1 15       |e8g8|black|\n",
      "|r2q1rk1/5ppp/p2p1b2/1p3P2/4R3/1B6/PPP2PPP/R2Q2K1 b - - 0 16        |f6b2|black|\n",
      "|r2q1rk1/5ppp/p2p4/1p3P2/4R3/1B6/PbP2PPP/1R1Q2K1 b - - 1 17         |b2f6|black|\n",
      "|r2q1rk1/5ppp/p2p1b2/1p1Q1P2/4R3/1B6/P1P2PPP/1R4K1 b - - 3 18       |a8c8|black|\n",
      "|2rq1rk1/1Q3ppp/p2p1b2/1p3P2/4R3/1B6/P1P2PPP/1R4K1 b - - 5 19       |c8c5|black|\n",
      "|3q1rk1/5ppp/Q2p1b2/1pr2P2/4R3/1B6/P1P2PPP/1R4K1 b - - 0 20         |c5f5|black|\n",
      "+-------------------------------------------------------------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, explode, col\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType\n",
    "import chess.pgn\n",
    "import io\n",
    "\n",
    "# UDF para extraer jugadas (FEN, movimiento, color)\n",
    "def pgn_a_jugadas(pgn_str, color):\n",
    "    resultado = []\n",
    "    try:\n",
    "        game = chess.pgn.read_game(io.StringIO(pgn_str))\n",
    "        if not game:\n",
    "            return []\n",
    "\n",
    "        board = game.board()\n",
    "        color_jugador = chess.WHITE if color == \"white\" else chess.BLACK\n",
    "\n",
    "        for move in game.mainline_moves():\n",
    "            if board.turn == color_jugador:\n",
    "                resultado.append((board.fen(), move.uci(), color))\n",
    "            board.push(move)\n",
    "    except:\n",
    "        pass\n",
    "    return resultado\n",
    "\n",
    "# Definir esquema\n",
    "esquema_jugada = ArrayType(StructType([\n",
    "    StructField(\"FEN\", StringType(), True),\n",
    "    StructField(\"Move\", StringType(), True),\n",
    "    StructField(\"Color\", StringType(), True)\n",
    "]))\n",
    "\n",
    "# Registrar UDF (parámetro adicional: color)\n",
    "pgn_udf = udf(lambda pgn: pgn_a_jugadas(pgn, \"white\"), esquema_jugada)\n",
    "pgn_udf_black = udf(lambda pgn: pgn_a_jugadas(pgn, \"black\"), esquema_jugada)\n",
    "\n",
    "# Aplicar UDF y explotar jugadas\n",
    "df_jugadas_blancas = nakamura_white_df.withColumn(\"jugadas\", explode(pgn_udf(col(\"pgn\")))) \\\n",
    "    .select(col(\"jugadas.FEN\"), col(\"jugadas.Move\"), col(\"jugadas.Color\"))\n",
    "\n",
    "df_jugadas_negras = nakamura_black_df.withColumn(\"jugadas\", explode(pgn_udf_black(col(\"pgn\")))) \\\n",
    "    .select(col(\"jugadas.FEN\"), col(\"jugadas.Move\"), col(\"jugadas.Color\"))\n",
    "\n",
    "# Unir ambos conjuntos\n",
    "# df_jugadas = df_jugadas_blancas.union(df_jugadas_negras)\n",
    "\n",
    "# Cachear para uso posterior con MLlib\n",
    "df_jugadas_blancas.cache()\n",
    "df_jugadas_blancas.show(truncate=False)\n",
    "\n",
    "df_jugadas_negras.cache()\n",
    "df_jugadas_negras.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63dce12-7265-4f6e-8f8c-3f82e3910c2d",
   "metadata": {},
   "source": [
    "⚠️ Consideraciones clave para adaptar tu flujo a Spark MLlib:\n",
    "MLlib no soporta tensores 3D (8x8x12) directamente.\n",
    "\n",
    "MLlib trabaja con Vector (denso o disperso), por lo tanto debes aplanar tu tensor a un vector de tamaño 8×8×12 = 768.\n",
    "\n",
    "No hay soporte nativo para LabelEncoder de sklearn, pero puedes hacer lo mismo con StringIndexer de Spark.\n",
    "\n",
    "UDFs con NumPy son posibles, pero deben devolver estructuras planas (List[Float]), no arrays 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36090948-27d5-4767-a275-63b1be5f36f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, DoubleType, IntegerType\n",
    "import numpy as np\n",
    "\n",
    "# Mapas globales para codificar piezas y movimientos\n",
    "piece_to_plane = {\n",
    "    'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
    "    'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11\n",
    "}\n",
    "\n",
    "# Codificar el tablero como vector 1D (768 elementos)\n",
    "def fen_to_vector(fen):\n",
    "    tensor = np.zeros((8, 8, 12), dtype=np.float32)\n",
    "    fen_board = fen.split(' ')[0]\n",
    "    rows = fen_board.split('/')\n",
    "    \n",
    "    for i, row in enumerate(rows):\n",
    "        col = 0\n",
    "        for char in row:\n",
    "            if char.isdigit():\n",
    "                col += int(char)\n",
    "            elif char in piece_to_plane:\n",
    "                plane = piece_to_plane[char]\n",
    "                tensor[i, col, plane] = 1\n",
    "                col += 1\n",
    "    return tensor.flatten().tolist()\n",
    "\n",
    "# Registrar como UDF en Spark\n",
    "fen_udf = udf(fen_to_vector, ArrayType(FloatType()))\n",
    "df_feat = df_jugadas.withColumn(\"features\", fen_udf(col(\"FEN\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d5c08f4-aa5b-4181-b399-d51dc350c0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Move\", outputCol=\"label\")\n",
    "df_final = indexer.fit(df_feat).transform(df_feat).select(\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3170514-6e76-4285-a8a3-7155bda1bc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f046d0a3-eb22-4eba-8744-e04d9995f06c",
   "metadata": {},
   "source": [
    "Lo que puedes hacer en entorno distribuido con Spark MLlib:\n",
    "MLlib no soporta CNNs ni tensores 4D. Su diseño está enfocado a:\n",
    "\n",
    "Modelos clásicos: árboles, regresión, SVM, redes densas básicas.\n",
    "\n",
    "Operación sobre vectores 1D (features) y etiquetas (label).\n",
    "\n",
    "Entrenamiento distribuido y escalable, pero no deep learning como en Keras.\n",
    "\n",
    "2. Alternativas si necesitas CNN en un entorno distribuido\n",
    "Aunque MLlib no soporta CNN, puedes usar frameworks especializados en deep learning que sí funcionan en entornos distribuidos:\n",
    "\n",
    "a) TensorFlow o PyTorch con Horovod o Spark\n",
    "Puedes integrar TensorFlow o PyTorch con Horovod, que permite entrenamiento distribuido sobre múltiples nodos.\n",
    "\n",
    "También puedes usar TensorFlowOnSpark o BigDL (una biblioteca de deep learning optimizada para Spark) para ejecutar redes neuronales sobre clústeres.\n",
    "\n",
    "b) BigDL\n",
    "BigDL es una alternativa poderosa si ya estás usando Spark y quieres hacer deep learning directamente sobre ese ecosistema.\n",
    "\n",
    "Soporta CNN, RNN y otros modelos complejos, y permite entrenamiento distribuido.\n",
    "\n",
    "Conclusión\n",
    "MLlib no es adecuado para CNN, pero puedes usar herramientas externas como TensorFlow + Horovod, BigDL o TensorFlowOnSpark si necesitas entrenamiento distribuido de redes neuronales profundas.\n",
    "\n",
    "⚠️ Limitación clave: Spark ≠ TensorFlow\n",
    "Spark y TensorFlow usan entornos de ejecución diferentes:\n",
    "\n",
    "Spark distribuye y mantiene df_final en su propio contexto de ejecución (con sus RDDs y DAGs).\n",
    "\n",
    "TensorFlow o Keras necesita los datos en forma de NumPy o Tensor en memoria local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cb20d90-f838-4d58-aed9-717de1180526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- label: double (nullable = false)\n",
      "\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[0.0, 0.0, 0.0, 0...| 29.0|\n",
      "+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.printSchema()\n",
    "df_final.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45011c0a-e100-416f-a55b-ebd452bce94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar DataFrame como Parquet en volumen compartido\n",
    "# df_final.write.mode(\"overwrite\").parquet(\"/shared/fen_jugadas.parquet\")   NO FUNCIONA\n",
    "df_pandas = df_final.toPandas()\n",
    "df_pandas.to_parquet(\"/shared/fen_jugadas.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43632be5-29c8-4db8-9af4-25a05632ef93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 84626 May 10 18:02 /shared/fen_jugadas.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -l /shared/fen_jugadas.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71d1598c-f5d2-4a01-a761-3d047e2f390c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/10 18:07:53 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "2025-05-10 18:07:55.153280: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-10 18:07:55.153276: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-10 18:07:55.371556: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-10 18:07:55.371770: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-10 18:07:55.372114: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-10 18:07:55.372114: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-10 18:07:57.091094: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-05-10 18:07:57.092431: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-05-10 18:07:58,949 INFO (MainThread-1612) connected to server at ('172.18.0.10', 39763)\n",
      "2025-05-10 18:07:58,949 INFO (MainThread-1612) TFSparkNode.reserve: {'executor_id': 1, 'host': '172.18.0.10', 'job_name': 'chief', 'task_index': 0, 'port': 34009, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-qh0joedc/listener-mtqdq1ug', 'authkey': b'%\\x0c:\\xf3\\x94`H\\x15\\xb3\\xef2)\\xd8U\\x1b0'}\n",
      "2025-05-10 18:07:59,114 INFO (MainThread-1615) connected to server at ('172.18.0.10', 39763)\n",
      "2025-05-10 18:07:59,115 INFO (MainThread-1615) TFSparkNode.reserve: {'executor_id': 0, 'host': '172.18.0.10', 'job_name': 'ps', 'task_index': 0, 'port': 34649, 'tb_pid': 0, 'tb_port': 0, 'addr': ('172.18.0.10', 45533), 'authkey': b'\\xbf\\xd6\\xfb\\xed[]N4\\xb0#\\xeerh\\xe8>\\xab'}\n",
      "2025-05-10 18:08:00,119 INFO (MainThread-1615) node: {'executor_id': 0, 'host': '172.18.0.10', 'job_name': 'ps', 'task_index': 0, 'port': 34649, 'tb_pid': 0, 'tb_port': 0, 'addr': ('172.18.0.10', 45533), 'authkey': b'\\xbf\\xd6\\xfb\\xed[]N4\\xb0#\\xeerh\\xe8>\\xab'}\n",
      "2025-05-10 18:08:00,119 INFO (MainThread-1615) node: {'executor_id': 1, 'host': '172.18.0.10', 'job_name': 'chief', 'task_index': 0, 'port': 34009, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-qh0joedc/listener-mtqdq1ug', 'authkey': b'%\\x0c:\\xf3\\x94`H\\x15\\xb3\\xef2)\\xd8U\\x1b0'}\n",
      "2025-05-10 18:08:00,119 INFO (MainThread-1615) export TF_CONFIG: {\"cluster\": {\"ps\": [\"172.18.0.10:34649\"], \"chief\": [\"172.18.0.10:34009\"]}, \"task\": {\"type\": \"ps\", \"index\": 0}, \"environment\": \"cloud\"}\n",
      "2025-05-10 18:08:00,133 INFO (MainThread-1615) Starting TensorFlow ps:0 as ps on cluster node 0 on background process\n",
      "2025-05-10 18:08:00,977 INFO (MainThread-1612) node: {'executor_id': 0, 'host': '172.18.0.10', 'job_name': 'ps', 'task_index': 0, 'port': 34649, 'tb_pid': 0, 'tb_port': 0, 'addr': ('172.18.0.10', 45533), 'authkey': b'\\xbf\\xd6\\xfb\\xed[]N4\\xb0#\\xeerh\\xe8>\\xab'}\n",
      "2025-05-10 18:08:00,977 INFO (MainThread-1612) node: {'executor_id': 1, 'host': '172.18.0.10', 'job_name': 'chief', 'task_index': 0, 'port': 34009, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-qh0joedc/listener-mtqdq1ug', 'authkey': b'%\\x0c:\\xf3\\x94`H\\x15\\xb3\\xef2)\\xd8U\\x1b0'}\n",
      "2025-05-10 18:08:00,978 INFO (MainThread-1612) export TF_CONFIG: {\"cluster\": {\"ps\": [\"172.18.0.10:34649\"], \"chief\": [\"172.18.0.10:34009\"]}, \"task\": {\"type\": \"chief\", \"index\": 0}, \"environment\": \"cloud\"}\n",
      "2025-05-10 18:08:01,056 INFO (MainThread-1612) Starting TensorFlow chief:0 on cluster node 1 on foreground thread\n",
      "Epoch 1/5\n",
      "Epoch 1/5\n",
      "90/90 [==============================] - 5s 35ms/step - loss: 6.7136 - accuracy: 0.0052 - val_loss: 6.5257 - val_accuracy: 0.0063\n",
      "Epoch 2/5\n",
      "90/90 [==============================] - 5s 31ms/step - loss: 6.7131 - accuracy: 0.0066 - val_loss: 6.5254 - val_accuracy: 0.0125\n",
      "Epoch 2/5\n",
      "90/90 [==============================] - 1s 16ms/step - loss: 6.3493 - accuracy: 0.0094 - val_loss: 6.5280 - val_accuracy: 0.0063\n",
      "Epoch 3/5\n",
      "90/90 [==============================] - 2s 18ms/step - loss: 6.3458 - accuracy: 0.0122 - val_loss: 6.5238 - val_accuracy: 0.0094\n",
      "Epoch 3/5\n",
      "90/90 [==============================] - 2s 17ms/step - loss: 6.2337 - accuracy: 0.0135 - val_loss: 6.5503 - val_accuracy: 0.0219\n",
      "Epoch 4/5\n",
      "90/90 [==============================] - 1s 16ms/step - loss: 6.2112 - accuracy: 0.0139 - val_loss: 6.5545 - val_accuracy: 0.0156\n",
      "Epoch 4/5\n",
      "90/90 [==============================] - 1s 16ms/step - loss: 6.1150 - accuracy: 0.0188 - val_loss: 6.6691 - val_accuracy: 0.0344\n",
      "Epoch 5/5\n",
      "90/90 [==============================] - 1s 15ms/step - loss: 6.0870 - accuracy: 0.0240 - val_loss: 6.5420 - val_accuracy: 0.0063\n",
      "Epoch 5/5\n",
      "90/90 [==============================] - 2s 21ms/step - loss: 5.9912 - accuracy: 0.0226 - val_loss: 6.6571 - val_accuracy: 0.0219\n",
      "90/90 [==============================] - 2s 21ms/step - loss: 5.9692 - accuracy: 0.0281 - val_loss: 6.6786 - val_accuracy: 0.0219\n",
      "INFO:tensorflow:Assets written to: /shared/modelo_distribuido/assets\n",
      "2025-05-10 18:08:15,811 INFO (MainThread-1612) Assets written to: /shared/modelo_distribuido/assets\n",
      "2025-05-10 18:08:15,828 INFO (MainThread-1612) Finished TensorFlow chief:0 on cluster node 1\n",
      "2025-05-10 18:08:35,087 INFO (MainThread-1612) Connected to TFSparkNode.mgr on 172.18.0.10, executor=1, state='running'\n",
      "2025-05-10 18:08:35,087 INFO (MainThread-1612) Stopping all queues\n",
      "2025-05-10 18:08:35,088 INFO (MainThread-1612) Feeding None into input queue\n",
      "2025-05-10 18:08:35,092 INFO (MainThread-1612) Feeding None into output queue\n",
      "2025-05-10 18:08:35,095 INFO (MainThread-1612) Setting mgr.state to 'stopped'\n",
      "2025-05-10 18:08:35,670 INFO (MainThread-1615) Got msg: None                    \n",
      "2025-05-10 18:08:35,670 INFO (MainThread-1615) Terminating ps\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ENTRENAMIENTO DISTRIBUIDO\n",
    "\n",
    "from tensorflowonspark import TFCluster, TFNode\n",
    "from pyspark.sql import SparkSession\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Parámetros\n",
    "data_path = \"/shared/fen_jugadas.parquet\"\n",
    "model_path = \"/shared/modelo_distribuido\"\n",
    "\n",
    "def model_fn():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(8, 8, 12)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def main_fun(args, ctx):\n",
    "    df = pd.read_parquet(args[\"data_path\"])\n",
    "    X = np.array(df[\"features\"].tolist()).reshape(-1, 8, 8, 12)\n",
    "    y = df[\"label\"].astype(np.int32).values\n",
    "\n",
    "    global NUM_CLASSES\n",
    "    NUM_CLASSES = len(np.unique(y))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "    model = model_fn()\n",
    "    model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), batch_size=32)\n",
    "\n",
    "    if ctx.job_name == \"chief\":\n",
    "        model.save(args[\"model_path\"])\n",
    "\n",
    "# Iniciar Spark y lanzar el cluster\n",
    "spark = SparkSession.builder.appName(\"TFoS-CNN\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "args = {\"data_path\": data_path, \"model_path\": model_path}\n",
    "\n",
    "cluster = TFCluster.run(sc, main_fun, args, num_executors=2, num_ps=1, input_mode=TFCluster.InputMode.TENSORFLOW, master_node=\"chief\", log_dir=\"/tmp/tf_logs\")\n",
    "cluster.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17b1f8e7-6f88-4cd2-8fab-5c4ecff7b918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 11:37:23.932746: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.0071 - loss: 6.8118 - val_accuracy: 0.0031 - val_loss: 6.4299\n",
      "Epoch 2/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0148 - loss: 6.3681 - val_accuracy: 0.0125 - val_loss: 6.4215\n",
      "Epoch 3/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.0171 - loss: 6.2490 - val_accuracy: 0.0094 - val_loss: 6.4129\n",
      "Epoch 4/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0202 - loss: 6.0973 - val_accuracy: 0.0250 - val_loss: 6.4572\n",
      "Epoch 5/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.0246 - loss: 5.9505 - val_accuracy: 0.0219 - val_loss: 6.5551\n",
      "Epoch 6/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.0329 - loss: 5.8137 - val_accuracy: 0.0312 - val_loss: 6.5765\n",
      "Epoch 7/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0437 - loss: 5.6680 - val_accuracy: 0.0344 - val_loss: 6.6006\n",
      "Epoch 8/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.0515 - loss: 5.3731 - val_accuracy: 0.0312 - val_loss: 7.0053\n",
      "Epoch 9/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0537 - loss: 5.1736 - val_accuracy: 0.0406 - val_loss: 7.0219\n",
      "Epoch 10/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.0538 - loss: 4.9638 - val_accuracy: 0.0500 - val_loss: 7.4603\n"
     ]
    }
   ],
   "source": [
    "## ENTRENAMIENTO LOCAL EN SPARK-CLIENT (USÉ EL ANTERIOR)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# Cargar datos exportados desde Spark\n",
    "df = pd.read_parquet(\"/shared/fen_jugadas.parquet\")\n",
    "\n",
    "# Convertir a tensores NumPy\n",
    "X = np.array(df[\"features\"].tolist()).reshape(-1, 8, 8, 12)\n",
    "y = df[\"label\"].values.astype(int)\n",
    "\n",
    "# Dividir en entrenamiento y validación\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Definir el modelo CNN\n",
    "model = Sequential([\n",
    "    Input(shape=(8, 8, 12)),\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(np.unique(y)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar y entrenar\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Guardar modelo\n",
    "model.save(\"/shared/modelo_nakamura.keras\")\n",
    "np.save(\"/shared/encoder.npy\", np.unique(y))  # Etiquetas numéricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b310df1-f86a-41e6-a71f-5634ae917f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d4447e-4d8f-44c1-84f9-96ba2de9e49a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3dc69e-2f71-4c9d-85b8-09defbb62665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ebfc8-8271-4c67-aada-416333531f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8095fc02-be1d-40de-9af5-52a6c5e17fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d36558-3eee-4175-9acf-bddc83c39835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abb744c1-09df-46fe-9f10-59eebca7cc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['features', 'label']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, IntegerType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Codificar FEN como vector plano 768 (8x8x12)\n",
    "def fen_to_flat_vector(fen):  \n",
    "    piece_to_plane = {\n",
    "        'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
    "        'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11\n",
    "    }\n",
    "    \n",
    "    tensor = np.zeros((8, 8, 12), dtype=np.float32)\n",
    "    fen_board = fen.split(' ')[0]\n",
    "    rows = fen_board.split('/')\n",
    "    \n",
    "    for i, row in enumerate(rows):\n",
    "        col = 0\n",
    "        for char in row:\n",
    "            if char.isdigit():\n",
    "                col += int(char)\n",
    "            elif char in piece_to_plane:\n",
    "                plane = piece_to_plane[char]\n",
    "                tensor[i, col, plane] = 1\n",
    "                col += 1\n",
    "    return tensor.flatten().tolist()\n",
    "\n",
    "# UDF para usar en Spark\n",
    "fen_udf = udf(fen_to_flat_vector, ArrayType(FloatType()))\n",
    "\n",
    "# Aplicar UDF para obtener features\n",
    "df_ml = df_jugadas.withColumn(\"features\", fen_udf(col(\"FEN\")))\n",
    "\n",
    "# Codificar movimiento como etiqueta\n",
    "indexer = StringIndexer(inputCol=\"Move\", outputCol=\"label\")\n",
    "indexer_model = indexer.fit(df_ml)\n",
    "df_ml = indexer_model.transform(df_ml)\n",
    "\n",
    "# Seleccionar columnas relevantes para MLlib\n",
    "df_final = df_ml.select(\"features\", \"label\")\n",
    "\n",
    "# Mostrar ejemplo\n",
    "# df_final.show(3, truncate=False)\n",
    "\n",
    "print(df_final.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f1e94c-447f-41c0-bc80-86018d7a615e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc69900-9fdb-4188-846c-5a649be2149f",
   "metadata": {},
   "source": [
    "## Nueva filosof√≠a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "134de9b2-cf56-4ca0-99b8-8ce878bf53c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, input_file_name\n",
    "from pyspark.sql.types import StringType\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a90933e2-a6d2-4e9c-aebb-d8a92d5b0a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/10 17:50:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"CargarPartidas\")\n",
    "    .master(\"local[*]\")  # \"yarn\"\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2545dae-23a0-4706-89af-3d63764c28a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/ajedrez/jugador\n",
      "Found 1 items\n",
      "drwxr-xr-x   - root supergroup          0 2025-05-09 11:46 /user/ajedrez/raw\n"
     ]
    }
   ],
   "source": [
    "# Por si se quiere eliminar alguna carpeta\n",
    "!hdfs dfs -rm -r /user/ajedrez/jugador\n",
    "# En principio, solo con la carpeta raw y las partidas dentro nos vale\n",
    "!hdfs dfs -ls /user/ajedrez/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b74c2df-3e02-4fd9-a3f1-47ff3a16e59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Leer todos los archivos como (ruta, contenido)\n",
    "rdd = spark.sparkContext.wholeTextFiles(\"hdfs:///user/ajedrez/raw/*.pgn\")\n",
    "\n",
    "# Dividir cada archivo en partidas\n",
    "def dividir_partidas_por_archivo(nombre_y_contenido):\n",
    "    ruta, contenido = nombre_y_contenido\n",
    "    partidas = re.split(r'\\n(?=\\[Event )', contenido)\n",
    "    return [(ruta, pgn) for pgn in partidas if \"[Event\" in pgn]\n",
    "\n",
    "rdd_partidas = rdd.flatMap(dividir_partidas_por_archivo)\n",
    "\n",
    "# Convertir a DataFrame\n",
    "df = rdd_partidas.toDF([\"archivo\", \"pgn\"])\n",
    "\n",
    "# UDFs para extraer tags\n",
    "def extraer_tag(tag, texto):\n",
    "    match = re.search(rf'\\[{tag} \"([^\"]+)\"\\]', texto)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "extraer_white = udf(lambda x: extraer_tag(\"White\", x), StringType())\n",
    "extraer_black = udf(lambda x: extraer_tag(\"Black\", x), StringType())\n",
    "extraer_date = udf(lambda x: extraer_tag(\"Date\", x), StringType())\n",
    "\n",
    "df = df.withColumn(\"white\", extraer_white(col(\"pgn\"))) \\\n",
    "       .withColumn(\"black\", extraer_black(col(\"pgn\"))) \\\n",
    "       .withColumn(\"date\", extraer_date(col(\"pgn\")))\n",
    "\n",
    "\n",
    "# Partidas donde Nakamura juega con blancas\n",
    "nakamura_white_df = df.filter(col(\"white\") == \"Nakamura,Hi\")\n",
    "\n",
    "# Partidas donde Nakamura juega con negras\n",
    "nakamura_black_df = df.filter(col(\"black\") == \"Nakamura,Hi\")\n",
    "\n",
    "# Guardar en HDFS\n",
    "# nakamura_white_df.select(\"pgn\").write.mode(\"overwrite\").text(\"hdfs:///user/ajedrez/jugador/Nakamura_blancas\")\n",
    "# nakamura_black_df.select(\"pgn\").write.mode(\"overwrite\").text(\"hdfs:///user/ajedrez/jugador/Nakamura_negras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e83401-bbca-4b47-83f9-822eb447b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En principio, solo con la carpeta raw y las partidas dentro nos vale\n",
    "# !hdfs dfs -ls /user/ajedrez/jugador/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f639b90-2fb8-4108-9498-be9cf0d7524a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (35, 5)\n",
      "['archivo', 'pgn', 'white', 'black', 'date']\n"
     ]
    }
   ],
   "source": [
    "filas = nakamura_white_df.count()\n",
    "columnas = len(nakamura_white_df.columns)\n",
    "print(f\"Shape: ({filas}, {columnas})\")\n",
    "\n",
    "print(nakamura_white_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ea8cc4-6fcd-40a7-9b07-0f32135f4667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+----------+\n",
      "|white      |black           |date      |\n",
      "+-----------+----------------+----------+\n",
      "|Nakamura,Hi|Vidit,S         |2024.04.05|\n",
      "|Nakamura,Hi|Praggnanandhaa,R|2024.04.07|\n",
      "|Nakamura,Hi|Nevednichy,V    |2024.04.02|\n",
      "|Nakamura,Hi|Eljanov,P       |2024.04.02|\n",
      "|Nakamura,Hi|Kuzubov,Y       |2024.04.02|\n",
      "|Nakamura,Hi|Kovalev,Vl      |2024.04.02|\n",
      "|Nakamura,Hi|Duda,J          |2024.04.02|\n",
      "|Nakamura,Hi|Makarian,Rudik  |2024.04.02|\n",
      "|Nakamura,Hi|Richter,Paul    |2024.04.02|\n",
      "|Nakamura,Hi|Durarbayli,Vasif|2024.04.02|\n",
      "+-----------+----------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nakamura_white_df.select(\"white\", \"black\", \"date\").show(truncate=False, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73fc26c-2342-47e8-a7f1-5b68468c9e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.select(\"pgn\").first()[\"pgn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25554bf2-ef29-4f77-a94f-b01f95b3b53a",
   "metadata": {},
   "source": [
    "### Se puede arreglar eso de separar en dos dataframes para luego unirlos en 1 es mejor no separarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "174ff74c-e519-4a9b-9508-196de716852a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------+----+-----+\n",
      "|FEN                                                                 |Move|Color|\n",
      "+--------------------------------------------------------------------+----+-----+\n",
      "|rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1            |e2e4|white|\n",
      "|rnbqkbnr/pppp1ppp/8/4p3/4P3/8/PPPP1PPP/RNBQKBNR w KQkq - 0 2        |g1f3|white|\n",
      "|r1bqkbnr/pppp1ppp/2n5/4p3/4P3/5N2/PPPP1PPP/RNBQKB1R w KQkq - 2 3    |f1b5|white|\n",
      "|r1bqkb1r/pppp1ppp/2n2n2/1B2p3/4P3/5N2/PPPP1PPP/RNBQK2R w KQkq - 4 4 |d2d3|white|\n",
      "|r1bqk2r/pppp1ppp/2n2n2/1Bb1p3/4P3/3P1N2/PPP2PPP/RNBQK2R w KQkq - 1 5|c2c3|white|\n",
      "|r1bq1rk1/pppp1ppp/2n2n2/1Bb1p3/4P3/2PP1N2/PP3PPP/RNBQK2R w KQ - 1 6 |e1g1|white|\n",
      "|r1bq1rk1/ppp2ppp/2np1n2/1Bb1p3/4P3/2PP1N2/PP3PPP/RNBQ1RK1 w - - 0 7 |h2h3|white|\n",
      "|r1bq1rk1/ppp1nppp/3p1n2/1Bb1p3/4P3/2PP1N1P/PP3PP1/RNBQ1RK1 w - - 1 8|d3d4|white|\n",
      "|r1bq1rk1/pp2nppp/2pp1n2/1Bb1p3/3PP3/2P2N1P/PP3PP1/RNBQ1RK1 w - - 0 9|b5d3|white|\n",
      "|r1bq1rk1/pp2nppp/1bpp1n2/4p3/3PP3/2PB1N1P/PP3PP1/RNBQ1RK1 w - - 2 10|d4e5|white|\n",
      "|r1bq1rk1/pp2nppp/1bp2n2/4p3/4P3/2PB1N1P/PP3PP1/RNBQ1RK1 w - - 0 11  |f3e5|white|\n",
      "|r2q1rk1/pp2nppp/1bp2n2/4N3/4P3/2PB3b/PP3PP1/RNBQ1RK1 w - - 0 12     |e5c4|white|\n",
      "|r2q1rk1/pp2nppp/1bp2n2/8/2N1P1b1/2PB4/PP3PP1/RNBQ1RK1 w - - 2 13    |d1c2|white|\n",
      "|r2q1rk1/ppb1nppp/2p2n2/8/2N1P1b1/2PB4/PPQ2PP1/RNB2RK1 w - - 4 14    |e4e5|white|\n",
      "|r2q1rk1/ppbnnppp/2p5/4P3/2N3b1/2PB4/PPQ2PP1/RNB2RK1 w - - 1 15      |d3h7|white|\n",
      "|r2q1r1k/ppbnnppB/2p5/4P3/2N3b1/2P5/PPQ2PP1/RNB2RK1 w - - 1 16       |h7d3|white|\n",
      "|r2q1r1k/p1bnnpp1/2p5/1p2P3/2N3b1/2PB4/PPQ2PP1/RNB2RK1 w - - 0 17    |c4e3|white|\n",
      "|r2q1r1k/p1b1npp1/2p5/1p2n3/6b1/2PBN3/PPQ2PP1/RNB2RK1 w - - 0 18     |d3e2|white|\n",
      "|r2q1r1k/p1b1n1p1/2p5/1p2np2/6b1/2P1N3/PPQ1BPP1/RNB2RK1 w - - 0 19   |f2f4|white|\n",
      "|r2q1r1k/p3n1p1/1bp5/1p2np2/5Pb1/2P1N3/PPQ1B1P1/RNB2RK1 w - - 1 20   |g1f2|white|\n",
      "+--------------------------------------------------------------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------------------------------------------------------------+----+-----+\n",
      "|FEN                                                                |Move|Color|\n",
      "+-------------------------------------------------------------------+----+-----+\n",
      "|rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1         |c7c5|black|\n",
      "|rnbqkbnr/pp1ppppp/8/2p5/4P3/5N2/PPPP1PPP/RNBQKB1R b KQkq - 1 2     |d7d6|black|\n",
      "|rnbqkbnr/pp2pppp/3p4/2p5/3PP3/5N2/PPP2PPP/RNBQKB1R b KQkq - 0 3    |c5d4|black|\n",
      "|rnbqkbnr/pp2pppp/3p4/8/3NP3/8/PPP2PPP/RNBQKB1R b KQkq - 0 4        |g8f6|black|\n",
      "|rnbqkb1r/pp2pppp/3p1n2/8/3NP3/2N5/PPP2PPP/R1BQKB1R b KQkq - 2 5    |e7e5|black|\n",
      "|rnbqkb1r/pp3ppp/3p1n2/1B2p3/3NP3/2N5/PPP2PPP/R1BQK2R b KQkq - 1 6  |b8d7|black|\n",
      "|r1bqkb1r/pp1n1ppp/3p1n2/1B2pN2/4P3/2N5/PPP2PPP/R1BQK2R b KQkq - 3 7|a7a6|black|\n",
      "|r1bqkb1r/1p1n1ppp/p2p1n2/4pN2/B3P3/2N5/PPP2PPP/R1BQK2R b KQkq - 1 8|b7b5|black|\n",
      "|r1bqkb1r/3n1ppp/p2p1n2/1p2pN2/4P3/1BN5/PPP2PPP/R1BQK2R b KQkq - 1 9|d7c5|black|\n",
      "|r1bqkb1r/5ppp/p2p1n2/1pn1pNB1/4P3/1BN5/PPP2PPP/R2QK2R b KQkq - 3 10|c8f5|black|\n",
      "|r2qkb1r/5ppp/p2p1n2/1pn1pPB1/8/1BN5/PPP2PPP/R2QK2R b KQkq - 0 11   |f8e7|black|\n",
      "|r2qk2r/4bppp/p2p1B2/1pn1pP2/8/1BN5/PPP2PPP/R2QK2R b KQkq - 0 12    |e7f6|black|\n",
      "|r2qk2r/5ppp/p2p1b2/1pn1pP2/8/1BN5/PPP2PPP/R2Q1RK1 b kq - 1 13      |e5e4|black|\n",
      "|r2qk2r/5ppp/p2p1b2/1pn2P2/4N3/1B6/PPP2PPP/R2Q1RK1 b kq - 0 14      |c5e4|black|\n",
      "|r2qk2r/5ppp/p2p1b2/1p3P2/4n3/1B6/PPP2PPP/R2QR1K1 b kq - 1 15       |e8g8|black|\n",
      "|r2q1rk1/5ppp/p2p1b2/1p3P2/4R3/1B6/PPP2PPP/R2Q2K1 b - - 0 16        |f6b2|black|\n",
      "|r2q1rk1/5ppp/p2p4/1p3P2/4R3/1B6/PbP2PPP/1R1Q2K1 b - - 1 17         |b2f6|black|\n",
      "|r2q1rk1/5ppp/p2p1b2/1p1Q1P2/4R3/1B6/P1P2PPP/1R4K1 b - - 3 18       |a8c8|black|\n",
      "|2rq1rk1/1Q3ppp/p2p1b2/1p3P2/4R3/1B6/P1P2PPP/1R4K1 b - - 5 19       |c8c5|black|\n",
      "|3q1rk1/5ppp/Q2p1b2/1pr2P2/4R3/1B6/P1P2PPP/1R4K1 b - - 0 20         |c5f5|black|\n",
      "+-------------------------------------------------------------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, explode, col\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType\n",
    "import chess.pgn\n",
    "import io\n",
    "\n",
    "# UDF para extraer jugadas (FEN, movimiento, color)\n",
    "def pgn_a_jugadas(pgn_str, color):\n",
    "    resultado = []\n",
    "    try:\n",
    "        game = chess.pgn.read_game(io.StringIO(pgn_str))\n",
    "        if not game:\n",
    "            return []\n",
    "\n",
    "        board = game.board()\n",
    "        color_jugador = chess.WHITE if color == \"white\" else chess.BLACK\n",
    "\n",
    "        for move in game.mainline_moves():\n",
    "            if board.turn == color_jugador:\n",
    "                resultado.append((board.fen(), move.uci(), color))\n",
    "            board.push(move)\n",
    "    except:\n",
    "        pass\n",
    "    return resultado\n",
    "\n",
    "# Definir esquema\n",
    "esquema_jugada = ArrayType(StructType([\n",
    "    StructField(\"FEN\", StringType(), True),\n",
    "    StructField(\"Move\", StringType(), True),\n",
    "    StructField(\"Color\", StringType(), True)\n",
    "]))\n",
    "\n",
    "# Registrar UDF (par√°metro adicional: color)\n",
    "pgn_udf = udf(lambda pgn: pgn_a_jugadas(pgn, \"white\"), esquema_jugada)\n",
    "pgn_udf_black = udf(lambda pgn: pgn_a_jugadas(pgn, \"black\"), esquema_jugada)\n",
    "\n",
    "# Aplicar UDF y explotar jugadas\n",
    "df_jugadas_blancas = nakamura_white_df.withColumn(\"jugadas\", explode(pgn_udf(col(\"pgn\")))) \\\n",
    "    .select(col(\"jugadas.FEN\"), col(\"jugadas.Move\"), col(\"jugadas.Color\"))\n",
    "\n",
    "df_jugadas_negras = nakamura_black_df.withColumn(\"jugadas\", explode(pgn_udf_black(col(\"pgn\")))) \\\n",
    "    .select(col(\"jugadas.FEN\"), col(\"jugadas.Move\"), col(\"jugadas.Color\"))\n",
    "\n",
    "# Unir ambos conjuntos\n",
    "# df_jugadas = df_jugadas_blancas.union(df_jugadas_negras)\n",
    "\n",
    "# Cachear para uso posterior con MLlib\n",
    "df_jugadas_blancas.cache()\n",
    "df_jugadas_blancas.show(truncate=False)\n",
    "\n",
    "df_jugadas_negras.cache()\n",
    "df_jugadas_negras.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63dce12-7265-4f6e-8f8c-3f82e3910c2d",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Consideraciones clave para adaptar tu flujo a Spark MLlib:\n",
    "MLlib no soporta tensores 3D (8x8x12) directamente.\n",
    "\n",
    "MLlib trabaja con Vector (denso o disperso), por lo tanto debes aplanar tu tensor a un vector de tama√±o 8√ó8√ó12 = 768.\n",
    "\n",
    "No hay soporte nativo para LabelEncoder de sklearn, pero puedes hacer lo mismo con StringIndexer de Spark.\n",
    "\n",
    "UDFs con NumPy son posibles, pero deben devolver estructuras planas (List[Float]), no arrays 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36090948-27d5-4767-a275-63b1be5f36f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, DoubleType, IntegerType\n",
    "import numpy as np\n",
    "\n",
    "# Mapas globales para codificar piezas y movimientos\n",
    "piece_to_plane = {\n",
    "    'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
    "    'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11\n",
    "}\n",
    "\n",
    "# Codificar el tablero como vector 1D (768 elementos)\n",
    "def fen_to_vector(fen):\n",
    "    tensor = np.zeros((8, 8, 12), dtype=np.float32)\n",
    "    fen_board = fen.split(' ')[0]\n",
    "    rows = fen_board.split('/')\n",
    "    \n",
    "    for i, row in enumerate(rows):\n",
    "        col = 0\n",
    "        for char in row:\n",
    "            if char.isdigit():\n",
    "                col += int(char)\n",
    "            elif char in piece_to_plane:\n",
    "                plane = piece_to_plane[char]\n",
    "                tensor[i, col, plane] = 1\n",
    "                col += 1\n",
    "    return tensor.flatten().tolist()\n",
    "\n",
    "# Registrar como UDF en Spark\n",
    "fen_udf = udf(fen_to_vector, ArrayType(FloatType()))\n",
    "df_feat = df_jugadas.withColumn(\"features\", fen_udf(col(\"FEN\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d5c08f4-aa5b-4181-b399-d51dc350c0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Move\", outputCol=\"label\")\n",
    "df_final = indexer.fit(df_feat).transform(df_feat).select(\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3170514-6e76-4285-a8a3-7155bda1bc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f046d0a3-eb22-4eba-8744-e04d9995f06c",
   "metadata": {},
   "source": [
    "Lo que puedes hacer en entorno distribuido con Spark MLlib:\n",
    "MLlib no soporta CNNs ni tensores 4D. Su dise√±o est√° enfocado a:\n",
    "\n",
    "Modelos cl√°sicos: √°rboles, regresi√≥n, SVM, redes densas b√°sicas.\n",
    "\n",
    "Operaci√≥n sobre vectores 1D (features) y etiquetas (label).\n",
    "\n",
    "Entrenamiento distribuido y escalable, pero no deep learning como en Keras.\n",
    "\n",
    "2. Alternativas si necesitas CNN en un entorno distribuido\n",
    "Aunque MLlib no soporta CNN, puedes usar frameworks especializados en deep learning que s√≠ funcionan en entornos distribuidos:\n",
    "\n",
    "a) TensorFlow o PyTorch con Horovod o Spark\n",
    "Puedes integrar TensorFlow o PyTorch con Horovod, que permite entrenamiento distribuido sobre m√∫ltiples nodos.\n",
    "\n",
    "Tambi√©n puedes usar TensorFlowOnSpark o BigDL (una biblioteca de deep learning optimizada para Spark) para ejecutar redes neuronales sobre cl√∫steres.\n",
    "\n",
    "b) BigDL\n",
    "BigDL es una alternativa poderosa si ya est√°s usando Spark y quieres hacer deep learning directamente sobre ese ecosistema.\n",
    "\n",
    "Soporta CNN, RNN y otros modelos complejos, y permite entrenamiento distribuido.\n",
    "\n",
    "Conclusi√≥n\n",
    "MLlib no es adecuado para CNN, pero puedes usar herramientas externas como TensorFlow + Horovod, BigDL o TensorFlowOnSpark si necesitas entrenamiento distribuido de redes neuronales profundas.\n",
    "\n",
    "‚ö†Ô∏è Limitaci√≥n clave: Spark ‚â† TensorFlow\n",
    "Spark y TensorFlow usan entornos de ejecuci√≥n diferentes:\n",
    "\n",
    "Spark distribuye y mantiene df_final en su propio contexto de ejecuci√≥n (con sus RDDs y DAGs).\n",
    "\n",
    "TensorFlow o Keras necesita los datos en forma de NumPy o Tensor en memoria local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cb20d90-f838-4d58-aed9-717de1180526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- label: double (nullable = false)\n",
      "\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[0.0, 0.0, 0.0, 0...| 29.0|\n",
      "+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.printSchema()\n",
    "df_final.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45011c0a-e100-416f-a55b-ebd452bce94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar DataFrame como Parquet en volumen compartido\n",
    "# df_final.write.mode(\"overwrite\").parquet(\"/shared/fen_jugadas.parquet\")   NO FUNCIONA\n",
    "df_pandas = df_final.toPandas()\n",
    "df_pandas.to_parquet(\"/shared/fen_jugadas.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43632be5-29c8-4db8-9af4-25a05632ef93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 84626 May 10 18:02 /shared/fen_jugadas.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -l /shared/fen_jugadas.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71d1598c-f5d2-4a01-a761-3d047e2f390c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/10 18:07:53 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "2025-05-10 18:07:55.153280: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-10 18:07:55.153276: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-10 18:07:55.371556: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-10 18:07:55.371770: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-10 18:07:55.372114: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-10 18:07:55.372114: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-10 18:07:57.091094: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-05-10 18:07:57.092431: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-05-10 18:07:58,949 INFO (MainThread-1612) connected to server at ('172.18.0.10', 39763)\n",
      "2025-05-10 18:07:58,949 INFO (MainThread-1612) TFSparkNode.reserve: {'executor_id': 1, 'host': '172.18.0.10', 'job_name': 'chief', 'task_index': 0, 'port': 34009, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-qh0joedc/listener-mtqdq1ug', 'authkey': b'%\\x0c:\\xf3\\x94`H\\x15\\xb3\\xef2)\\xd8U\\x1b0'}\n",
      "2025-05-10 18:07:59,114 INFO (MainThread-1615) connected to server at ('172.18.0.10', 39763)\n",
      "2025-05-10 18:07:59,115 INFO (MainThread-1615) TFSparkNode.reserve: {'executor_id': 0, 'host': '172.18.0.10', 'job_name': 'ps', 'task_index': 0, 'port': 34649, 'tb_pid': 0, 'tb_port': 0, 'addr': ('172.18.0.10', 45533), 'authkey': b'\\xbf\\xd6\\xfb\\xed[]N4\\xb0#\\xeerh\\xe8>\\xab'}\n",
      "2025-05-10 18:08:00,119 INFO (MainThread-1615) node: {'executor_id': 0, 'host': '172.18.0.10', 'job_name': 'ps', 'task_index': 0, 'port': 34649, 'tb_pid': 0, 'tb_port': 0, 'addr': ('172.18.0.10', 45533), 'authkey': b'\\xbf\\xd6\\xfb\\xed[]N4\\xb0#\\xeerh\\xe8>\\xab'}\n",
      "2025-05-10 18:08:00,119 INFO (MainThread-1615) node: {'executor_id': 1, 'host': '172.18.0.10', 'job_name': 'chief', 'task_index': 0, 'port': 34009, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-qh0joedc/listener-mtqdq1ug', 'authkey': b'%\\x0c:\\xf3\\x94`H\\x15\\xb3\\xef2)\\xd8U\\x1b0'}\n",
      "2025-05-10 18:08:00,119 INFO (MainThread-1615) export TF_CONFIG: {\"cluster\": {\"ps\": [\"172.18.0.10:34649\"], \"chief\": [\"172.18.0.10:34009\"]}, \"task\": {\"type\": \"ps\", \"index\": 0}, \"environment\": \"cloud\"}\n",
      "2025-05-10 18:08:00,133 INFO (MainThread-1615) Starting TensorFlow ps:0 as ps on cluster node 0 on background process\n",
      "2025-05-10 18:08:00,977 INFO (MainThread-1612) node: {'executor_id': 0, 'host': '172.18.0.10', 'job_name': 'ps', 'task_index': 0, 'port': 34649, 'tb_pid': 0, 'tb_port': 0, 'addr': ('172.18.0.10', 45533), 'authkey': b'\\xbf\\xd6\\xfb\\xed[]N4\\xb0#\\xeerh\\xe8>\\xab'}\n",
      "2025-05-10 18:08:00,977 INFO (MainThread-1612) node: {'executor_id': 1, 'host': '172.18.0.10', 'job_name': 'chief', 'task_index': 0, 'port': 34009, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-qh0joedc/listener-mtqdq1ug', 'authkey': b'%\\x0c:\\xf3\\x94`H\\x15\\xb3\\xef2)\\xd8U\\x1b0'}\n",
      "2025-05-10 18:08:00,978 INFO (MainThread-1612) export TF_CONFIG: {\"cluster\": {\"ps\": [\"172.18.0.10:34649\"], \"chief\": [\"172.18.0.10:34009\"]}, \"task\": {\"type\": \"chief\", \"index\": 0}, \"environment\": \"cloud\"}\n",
      "2025-05-10 18:08:01,056 INFO (MainThread-1612) Starting TensorFlow chief:0 on cluster node 1 on foreground thread\n",
      "Epoch 1/5\n",
      "Epoch 1/5\n",
      "90/90 [==============================] - 5s 35ms/step - loss: 6.7136 - accuracy: 0.0052 - val_loss: 6.5257 - val_accuracy: 0.0063\n",
      "Epoch 2/5\n",
      "90/90 [==============================] - 5s 31ms/step - loss: 6.7131 - accuracy: 0.0066 - val_loss: 6.5254 - val_accuracy: 0.0125\n",
      "Epoch 2/5\n",
      "90/90 [==============================] - 1s 16ms/step - loss: 6.3493 - accuracy: 0.0094 - val_loss: 6.5280 - val_accuracy: 0.0063\n",
      "Epoch 3/5\n",
      "90/90 [==============================] - 2s 18ms/step - loss: 6.3458 - accuracy: 0.0122 - val_loss: 6.5238 - val_accuracy: 0.0094\n",
      "Epoch 3/5\n",
      "90/90 [==============================] - 2s 17ms/step - loss: 6.2337 - accuracy: 0.0135 - val_loss: 6.5503 - val_accuracy: 0.0219\n",
      "Epoch 4/5\n",
      "90/90 [==============================] - 1s 16ms/step - loss: 6.2112 - accuracy: 0.0139 - val_loss: 6.5545 - val_accuracy: 0.0156\n",
      "Epoch 4/5\n",
      "90/90 [==============================] - 1s 16ms/step - loss: 6.1150 - accuracy: 0.0188 - val_loss: 6.6691 - val_accuracy: 0.0344\n",
      "Epoch 5/5\n",
      "90/90 [==============================] - 1s 15ms/step - loss: 6.0870 - accuracy: 0.0240 - val_loss: 6.5420 - val_accuracy: 0.0063\n",
      "Epoch 5/5\n",
      "90/90 [==============================] - 2s 21ms/step - loss: 5.9912 - accuracy: 0.0226 - val_loss: 6.6571 - val_accuracy: 0.0219\n",
      "90/90 [==============================] - 2s 21ms/step - loss: 5.9692 - accuracy: 0.0281 - val_loss: 6.6786 - val_accuracy: 0.0219\n",
      "INFO:tensorflow:Assets written to: /shared/modelo_distribuido/assets\n",
      "2025-05-10 18:08:15,811 INFO (MainThread-1612) Assets written to: /shared/modelo_distribuido/assets\n",
      "2025-05-10 18:08:15,828 INFO (MainThread-1612) Finished TensorFlow chief:0 on cluster node 1\n",
      "2025-05-10 18:08:35,087 INFO (MainThread-1612) Connected to TFSparkNode.mgr on 172.18.0.10, executor=1, state='running'\n",
      "2025-05-10 18:08:35,087 INFO (MainThread-1612) Stopping all queues\n",
      "2025-05-10 18:08:35,088 INFO (MainThread-1612) Feeding None into input queue\n",
      "2025-05-10 18:08:35,092 INFO (MainThread-1612) Feeding None into output queue\n",
      "2025-05-10 18:08:35,095 INFO (MainThread-1612) Setting mgr.state to 'stopped'\n",
      "2025-05-10 18:08:35,670 INFO (MainThread-1615) Got msg: None                    \n",
      "2025-05-10 18:08:35,670 INFO (MainThread-1615) Terminating ps\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ENTRENAMIENTO DISTRIBUIDO\n",
    "\n",
    "from tensorflowonspark import TFCluster, TFNode\n",
    "from pyspark.sql import SparkSession\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Par√°metros\n",
    "data_path = \"/shared/fen_jugadas.parquet\"\n",
    "model_path = \"/shared/modelo_distribuido\"\n",
    "\n",
    "def model_fn():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(8, 8, 12)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def main_fun(args, ctx):\n",
    "    df = pd.read_parquet(args[\"data_path\"])\n",
    "    X = np.array(df[\"features\"].tolist()).reshape(-1, 8, 8, 12)\n",
    "    y = df[\"label\"].astype(np.int32).values\n",
    "\n",
    "    global NUM_CLASSES\n",
    "    NUM_CLASSES = len(np.unique(y))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "    model = model_fn()\n",
    "    model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), batch_size=32)\n",
    "\n",
    "    if ctx.job_name == \"chief\":\n",
    "        model.save(args[\"model_path\"])\n",
    "\n",
    "# Iniciar Spark y lanzar el cluster\n",
    "spark = SparkSession.builder.appName(\"TFoS-CNN\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "args = {\"data_path\": data_path, \"model_path\": model_path}\n",
    "\n",
    "cluster = TFCluster.run(sc, main_fun, args, num_executors=2, num_ps=1, input_mode=TFCluster.InputMode.TENSORFLOW, master_node=\"chief\", log_dir=\"/tmp/tf_logs\")\n",
    "cluster.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17b1f8e7-6f88-4cd2-8fab-5c4ecff7b918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 11:37:23.932746: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.0071 - loss: 6.8118 - val_accuracy: 0.0031 - val_loss: 6.4299\n",
      "Epoch 2/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0148 - loss: 6.3681 - val_accuracy: 0.0125 - val_loss: 6.4215\n",
      "Epoch 3/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.0171 - loss: 6.2490 - val_accuracy: 0.0094 - val_loss: 6.4129\n",
      "Epoch 4/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0202 - loss: 6.0973 - val_accuracy: 0.0250 - val_loss: 6.4572\n",
      "Epoch 5/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.0246 - loss: 5.9505 - val_accuracy: 0.0219 - val_loss: 6.5551\n",
      "Epoch 6/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.0329 - loss: 5.8137 - val_accuracy: 0.0312 - val_loss: 6.5765\n",
      "Epoch 7/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0437 - loss: 5.6680 - val_accuracy: 0.0344 - val_loss: 6.6006\n",
      "Epoch 8/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.0515 - loss: 5.3731 - val_accuracy: 0.0312 - val_loss: 7.0053\n",
      "Epoch 9/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0537 - loss: 5.1736 - val_accuracy: 0.0406 - val_loss: 7.0219\n",
      "Epoch 10/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.0538 - loss: 4.9638 - val_accuracy: 0.0500 - val_loss: 7.4603\n"
     ]
    }
   ],
   "source": [
    "## ENTRENAMIENTO LOCAL EN SPARK-CLIENT (US√â EL ANTERIOR)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# Cargar datos exportados desde Spark\n",
    "df = pd.read_parquet(\"/shared/fen_jugadas.parquet\")\n",
    "\n",
    "# Convertir a tensores NumPy\n",
    "X = np.array(df[\"features\"].tolist()).reshape(-1, 8, 8, 12)\n",
    "y = df[\"label\"].values.astype(int)\n",
    "\n",
    "# Dividir en entrenamiento y validaci√≥n\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Definir el modelo CNN\n",
    "model = Sequential([\n",
    "    Input(shape=(8, 8, 12)),\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(np.unique(y)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar y entrenar\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Guardar modelo\n",
    "model.save(\"/shared/modelo_nakamura.keras\")\n",
    "np.save(\"/shared/encoder.npy\", np.unique(y))  # Etiquetas num√©ricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b310df1-f86a-41e6-a71f-5634ae917f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d4447e-4d8f-44c1-84f9-96ba2de9e49a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3dc69e-2f71-4c9d-85b8-09defbb62665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ebfc8-8271-4c67-aada-416333531f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8095fc02-be1d-40de-9af5-52a6c5e17fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d36558-3eee-4175-9acf-bddc83c39835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abb744c1-09df-46fe-9f10-59eebca7cc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['features', 'label']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, IntegerType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Codificar FEN como vector plano 768 (8x8x12)\n",
    "def fen_to_flat_vector(fen):  \n",
    "    piece_to_plane = {\n",
    "        'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
    "        'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11\n",
    "    }\n",
    "    \n",
    "    tensor = np.zeros((8, 8, 12), dtype=np.float32)\n",
    "    fen_board = fen.split(' ')[0]\n",
    "    rows = fen_board.split('/')\n",
    "    \n",
    "    for i, row in enumerate(rows):\n",
    "        col = 0\n",
    "        for char in row:\n",
    "            if char.isdigit():\n",
    "                col += int(char)\n",
    "            elif char in piece_to_plane:\n",
    "                plane = piece_to_plane[char]\n",
    "                tensor[i, col, plane] = 1\n",
    "                col += 1\n",
    "    return tensor.flatten().tolist()\n",
    "\n",
    "# UDF para usar en Spark\n",
    "fen_udf = udf(fen_to_flat_vector, ArrayType(FloatType()))\n",
    "\n",
    "# Aplicar UDF para obtener features\n",
    "df_ml = df_jugadas.withColumn(\"features\", fen_udf(col(\"FEN\")))\n",
    "\n",
    "# Codificar movimiento como etiqueta\n",
    "indexer = StringIndexer(inputCol=\"Move\", outputCol=\"label\")\n",
    "indexer_model = indexer.fit(df_ml)\n",
    "df_ml = indexer_model.transform(df_ml)\n",
    "\n",
    "# Seleccionar columnas relevantes para MLlib\n",
    "df_final = df_ml.select(\"features\", \"label\")\n",
    "\n",
    "# Mostrar ejemplo\n",
    "# df_final.show(3, truncate=False)\n",
    "\n",
    "print(df_final.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f1e94c-447f-41c0-bc80-86018d7a615e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
